{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Building a Spam Filter with Naive Bayes\n",
    "\n",
    "#### The objective of this project is to apply Bayes Theorem to create an algorithm with > 80% accuracy to sort out spam messages.\n",
    "The dataset can be downloaded [here](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). For convention, messages are classified as either \"spam\" or \"ham\".\n",
    "\n",
    "### Methodology\n",
    "#### 1. Importing and inspecting the data\n",
    "#### 2. Creating Training and Testing sets\n",
    "#### 3. Data Cleaning and Processing\n",
    "#### 4. Creating the Spam Filter\n",
    "#### 5. Classifying new messages\n",
    "\n",
    "### Conclusion: Developed algorith had 98% accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Importing and inspecting the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original dataset: 5572 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset: {} rows and {} columns'.format(data.shape[0], data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Spam entries: 87%\n\nHam entries: 13%\n"
     ]
    }
   ],
   "source": [
    "print(\"Spam entries: {0:.0%}\".format(data['Label'].value_counts(normalize=True)[0]))\n",
    "print('')\n",
    "print(\"Ham entries: {0:.0%}\".format(data['Label'].value_counts(normalize=True)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "87%\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:.0%}\".format(data['Label'].value_counts(normalize=True)[0]))"
   ]
  },
  {
   "source": [
    "# 2. Creating Training and Testing sets\n",
    "### The first step is to randomize the entire dataset to ensure that spam and ham messages are spread properly.\n",
    "\n",
    "### In sequence, the 5.572 messages will be divided as follows:\n",
    "1) Training Set (80% = 4.458 messages)\n",
    "\n",
    "2) Testing Set (20% = 1.114 messages)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train data: 4458 rows and 2 columns\n\nTest data: 1114 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "randomized_data = data.sample(frac=1, random_state=1)\n",
    "\n",
    "split_index = round(len(randomized_data) * 0.8)\n",
    "\n",
    "train_data = randomized_data[:split_index].reset_index(drop=True)\n",
    "\n",
    "test_data = randomized_data[split_index:].reset_index(drop=True)\n",
    "\n",
    "print('Train data: {} rows and {} columns'.format(train_data.shape[0], train_data.shape[1]))\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Test data: {} rows and {} columns'.format(test_data.shape[0], test_data.shape[1]))"
   ]
  },
  {
   "source": [
    "### Now we must verify the distribution of spam and ham entries in both datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_data Spam entries: 87%\n\ntrain_data Ham entries: 13%\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data Spam entries: {0:.0%}\".format(train_data['Label'].value_counts(normalize=True)[0]))\n",
    "print('')\n",
    "print(\"train_data Ham entries: {0:.0%}\".format(train_data['Label'].value_counts(normalize=True)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test_data Spam entries: 87%\n\ntest_data Ham entries: 13%\n"
     ]
    }
   ],
   "source": [
    "print(\"test_data Spam entries: {0:.0%}\".format(test_data['Label'].value_counts(normalize=True)[0]))\n",
    "print('')\n",
    "print(\"test_data Ham entries: {0:.0%}\".format(test_data['Label'].value_counts(normalize=True)[1]))"
   ]
  },
  {
   "source": [
    "### Checking values in greater detail before proceeding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_data:\nham     0.86541\nspam    0.13459\nName: Label, dtype: float64\n\ntest_data:\nham     0.868043\nspam    0.131957\nName: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data:\")\n",
    "print(train_data['Label'].value_counts(normalize=True))\n",
    "\n",
    "print('')\n",
    "\n",
    "print(\"test_data:\")\n",
    "print(test_data['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "source": [
    "# 3. Data Cleaning and Processing\n",
    "\n",
    "### In this step we'll lowercase all messages and remove characters that have no statistical significance for the algorithm (eg. ponctuations)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       Yep, by the pretty sculpture\n",
       "1   ham      Yes, princess. Are you going to make me moan?\n",
       "2   ham                         Welp apparently he retired\n",
       "3   ham                                            Havent.\n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Yep, by the pretty sculpture</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Yes, princess. Are you going to make me moan?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Welp apparently he retired</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Havent.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# Checking dataset before processing\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       yep  by the pretty sculpture\n",
       "1   ham      yes  princess  are you going to make me moan \n",
       "2   ham                         welp apparently he retired\n",
       "3   ham                                            havent \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>yep  by the pretty sculpture</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>yes  princess  are you going to make me moan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>welp apparently he retired</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>havent</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# Removing punctuations and special characters\n",
    "\n",
    "train_data['SMS'] = train_data['SMS'].str.replace('\\W', ' ')\n",
    "\n",
    "train_data['SMS'] = train_data['SMS'].str.lower()\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of unique words: 7783\n"
     ]
    }
   ],
   "source": [
    "# Creating a list ('vocabulary') to store unique words\n",
    " \n",
    "vocabulary = []\n",
    "\n",
    "train_data['SMS'] = train_data['SMS'].str.split()\n",
    "\n",
    "for x in train_data['SMS']:\n",
    "    for y in x:\n",
    "        vocabulary.append(y)\n",
    "\n",
    "vocabulary = list(set(vocabulary))\n",
    "\n",
    "print(\"Number of unique words: {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   purple  reality  09099725823  instructions  ...  othrwise  while  intentions  epi\n",
       "0       0        0            0             0  ...         0      0           0    0\n",
       "1       0        0            0             0  ...         0      0           0    0\n",
       "2       0        0            0             0  ...         0      0           0    0\n",
       "3       0        0            0             0  ...         0      0           0    0\n",
       "4       0        0            0             0  ...         0      0           0    0\n",
       "\n",
       "[5 rows x 7783 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>purple</th>\n      <th>reality</th>\n      <th>09099725823</th>\n      <th>instructions</th>\n      <th>transaction</th>\n      <th>adsense</th>\n      <th>78</th>\n      <th>def</th>\n      <th>seen</th>\n      <th>her</th>\n      <th>im</th>\n      <th>yours</th>\n      <th>sao</th>\n      <th>trishul</th>\n      <th>twins</th>\n      <th>tmorrow</th>\n      <th>aretaking</th>\n      <th>150</th>\n      <th>hiya</th>\n      <th>library</th>\n      <th>forward</th>\n      <th>dysentry</th>\n      <th>iwana</th>\n      <th>plum</th>\n      <th>o2fwd</th>\n      <th>ha</th>\n      <th>convince</th>\n      <th>opener</th>\n      <th>trip</th>\n      <th>3510i</th>\n      <th>gist</th>\n      <th>partnership</th>\n      <th>fletcher</th>\n      <th>bought</th>\n      <th>ecstasy</th>\n      <th>wating</th>\n      <th>flirt</th>\n      <th>route</th>\n      <th>which</th>\n      <th>feb</th>\n      <th>...</th>\n      <th>434</th>\n      <th>howu</th>\n      <th>university</th>\n      <th>bcm</th>\n      <th>tobacco</th>\n      <th>humanities</th>\n      <th>petexxx</th>\n      <th>placed</th>\n      <th>choice</th>\n      <th>gain</th>\n      <th>vital</th>\n      <th>os</th>\n      <th>option</th>\n      <th>spiffing</th>\n      <th>treacle</th>\n      <th>uv</th>\n      <th>yun</th>\n      <th>8077</th>\n      <th>computer</th>\n      <th>helen</th>\n      <th>cupboard</th>\n      <th>sang</th>\n      <th>cruise</th>\n      <th>apartment</th>\n      <th>sayy</th>\n      <th>ore</th>\n      <th>disc</th>\n      <th>connected</th>\n      <th>had</th>\n      <th>clearing</th>\n      <th>poet</th>\n      <th>fetch</th>\n      <th>xin</th>\n      <th>bettr</th>\n      <th>dhina</th>\n      <th>cough</th>\n      <th>othrwise</th>\n      <th>while</th>\n      <th>intentions</th>\n      <th>epi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 7783 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "# Counting the frequency of each unique word\n",
    "\n",
    "word_counts_per_sms = {unique_word:[0] * len(train_data['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(train_data['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "\n",
    "# Transforming the frequency dictionary into a DataFrame\n",
    "\n",
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "\n",
    "# This DataFrame has 7783 columns (one for each unique word) and 4458 rows (one for each index (row) in train_data)\n",
    "\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS  ...  intentions  epi\n",
       "0   ham                  [yep, by, the, pretty, sculpture]  ...           0    0\n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...  ...           0    0\n",
       "2   ham                    [welp, apparently, he, retired]  ...           0    0\n",
       "3   ham                                           [havent]  ...           0    0\n",
       "4   ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...  ...           0    0\n",
       "\n",
       "[5 rows x 7785 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n      <th>purple</th>\n      <th>reality</th>\n      <th>09099725823</th>\n      <th>instructions</th>\n      <th>transaction</th>\n      <th>adsense</th>\n      <th>78</th>\n      <th>def</th>\n      <th>seen</th>\n      <th>her</th>\n      <th>im</th>\n      <th>yours</th>\n      <th>sao</th>\n      <th>trishul</th>\n      <th>twins</th>\n      <th>tmorrow</th>\n      <th>aretaking</th>\n      <th>150</th>\n      <th>hiya</th>\n      <th>library</th>\n      <th>forward</th>\n      <th>dysentry</th>\n      <th>iwana</th>\n      <th>plum</th>\n      <th>o2fwd</th>\n      <th>ha</th>\n      <th>convince</th>\n      <th>opener</th>\n      <th>trip</th>\n      <th>3510i</th>\n      <th>gist</th>\n      <th>partnership</th>\n      <th>fletcher</th>\n      <th>bought</th>\n      <th>ecstasy</th>\n      <th>wating</th>\n      <th>flirt</th>\n      <th>route</th>\n      <th>...</th>\n      <th>434</th>\n      <th>howu</th>\n      <th>university</th>\n      <th>bcm</th>\n      <th>tobacco</th>\n      <th>humanities</th>\n      <th>petexxx</th>\n      <th>placed</th>\n      <th>choice</th>\n      <th>gain</th>\n      <th>vital</th>\n      <th>os</th>\n      <th>option</th>\n      <th>spiffing</th>\n      <th>treacle</th>\n      <th>uv</th>\n      <th>yun</th>\n      <th>8077</th>\n      <th>computer</th>\n      <th>helen</th>\n      <th>cupboard</th>\n      <th>sang</th>\n      <th>cruise</th>\n      <th>apartment</th>\n      <th>sayy</th>\n      <th>ore</th>\n      <th>disc</th>\n      <th>connected</th>\n      <th>had</th>\n      <th>clearing</th>\n      <th>poet</th>\n      <th>fetch</th>\n      <th>xin</th>\n      <th>bettr</th>\n      <th>dhina</th>\n      <th>cough</th>\n      <th>othrwise</th>\n      <th>while</th>\n      <th>intentions</th>\n      <th>epi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>[yep, by, the, pretty, sculpture]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>[yes, princess, are, you, going, to, make, me,...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>[welp, apparently, he, retired]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>[havent]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 7785 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "# Concatenating train_data and word_counts\n",
    "\n",
    "train_data_clean = pd.concat([train_data, word_counts], axis = 1)\n",
    "\n",
    "train_data_clean.head()"
   ]
  },
  {
   "source": [
    "# 4. Creating the Spam Filter\n",
    "\n",
    "### We need to calculate some probabilities to deploy Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking spam and ham into two DataFrames\n",
    "spam_messages = train_data_clean[train_data_clean['Label'] == 'spam']\n",
    "ham_messages = train_data_clean[train_data_clean['Label'] == 'ham']\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "# Probabilities of ham and spam\n",
    "p_spam = len(spam_messages) / len(train_data_clean)\n",
    "p_ham = len(ham_messages) / len(train_data_clean)\n",
    "\n",
    "n_words_per_spam_message = spam_messages['SMS'].apply(len)\n",
    "n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "n_words_per_ham_message = ham_messages['SMS'].apply(len)\n",
    "n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "n_vocabulary = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the probability of each word in the vocabulary given the message is ham or spam\n",
    "\n",
    "parameters_spam = {unique_word: 0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_words: 0 for unique_words in vocabulary}\n",
    "\n",
    "for word in vocabulary:\n",
    "    \n",
    "    n_word_given_spam = spam_messages[word].sum()\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "    n_word_given_ham = ham_messages[word].sum()\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "source": [
    "# 5. Classifying new messages\n",
    "### We have all parameters necessary for our algorithm to work\n",
    "The filter can be understood as a function that:\n",
    "\n",
    "1. Takes in as input a new message(word 1, word 2, ..., word n)\n",
    "\n",
    "2. Calculates P(Spam|word 1, word 2, ..., word n) and P(Ham|word 1, word 2, ..., word n)\n",
    "\n",
    "3. Compares the two probabilities and classifies the message according to which is highest\n",
    "\n",
    "4. If P(Ham) = P(Spam), the algorithm will request human assistance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "\n",
    "    message = message.replace('\\W', ' ')\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'Human classification required'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "# Test 1\n",
    "\n",
    "classify(\"WINNER!! This is the secret code to unlock the money: C3421.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "# Test 2\n",
    "\n",
    "classify(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "source": [
    "# Checking test_data\n",
    "\n",
    "test_data.head()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 95,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham          Later i guess. I needa do mcat study too.\n",
       "1   ham             But i haf enuff space got like 4 mb...\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...\n",
       "4   ham  All done, all handed in. Don't know if mega sh..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Later i guess. I needa do mcat study too.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>But i haf enuff space got like 4 mb...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>All sounds good. Fingers . Makes it difficult ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>All done, all handed in. Don't know if mega sh...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 95
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS predicted\n",
       "0   ham          Later i guess. I needa do mcat study too.       ham\n",
       "1   ham             But i haf enuff space got like 4 mb...       ham\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...      spam\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...       ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...       ham"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>SMS</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Later i guess. I needa do mcat study too.</td>\n      <td>ham</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>But i haf enuff space got like 4 mb...</td>\n      <td>ham</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n      <td>spam</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>All sounds good. Fingers . Makes it difficult ...</td>\n      <td>ham</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>All done, all handed in. Don't know if mega sh...</td>\n      <td>ham</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "# Applying the algorithm to test_data\n",
    "\n",
    "test_data['predicted'] = test_data['SMS'].apply(classify)\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correct predictions: 1095\nIncorrect predictions: 19\nAlgorithm Accuracy: 98%\n"
     ]
    }
   ],
   "source": [
    "# Measuring algorithm accuracy\n",
    "\n",
    "correct = 0\n",
    "\n",
    "total = len(test_data)\n",
    "\n",
    "for row in test_data.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['predicted']:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = (correct / total)\n",
    "\n",
    "print(\"Correct predictions: {}\".format(correct))\n",
    "print(\"Incorrect predictions: {}\".format(total - correct))\n",
    "print(\"Algorithm Accuracy: {0:.0%}\".format(accuracy))"
   ]
  }
 ]
}